{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ranabilal09/Storm-Research/blob/main/Storm_Research.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "qnkeFOsdrVBJ"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet -U langchain_community langchain_openai langchain_huggingface langchain-groq langchain_fireworks langgraph wikipedia duckduckgo-search tavily-python langchain-google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "15zoK2SOsmlu"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "\n",
        "os.environ[\"GOGGLE_API_KEY\"] = userdata.get(\"Gemini_Api_Key\")\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get(\"tavily_api_key\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get(\"langchai_api_key\")\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Storm-Research\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"GROQ_API_KEY\"] = userdata.get(\"groq_api_key\")\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get(\"HF_TOKEN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "SWWnN9WAsR6w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605639f5-5418-4526-a290-427f2a32be4e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"The capital of France is Paris. It's located in the north-central part of the country and is one of the most populous urban areas in Europe. Paris is known for its iconic landmarks such as the Eiffel Tower, Louvre Museum, Notre-Dame Cathedral, and Champs-Élysées. It is also famous for its influence in fashion, gastronomy, art, and culture.\", additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 15, 'total_tokens': 107, 'completion_time': 0.144599778, 'prompt_time': 0.002166594, 'queue_time': 0.011555954, 'total_time': 0.146766372}, 'model_name': 'mixtral-8x7b-32768', 'system_fingerprint': 'fp_c5f20b5bb1', 'finish_reason': 'stop', 'logprobs': None}, id='run-77d72c2a-aac3-4cd2-918f-894f68f7c5ca-0', usage_metadata={'input_tokens': 15, 'output_tokens': 92, 'total_tokens': 107})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_groq import ChatGroq\n",
        "from langchain_huggingface import HuggingFaceEndpoint\n",
        "\n",
        "model = HuggingFaceEndpoint(model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "fast_llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
        "llm = ChatGroq(model=\"mixtral-8x7b-32768\")\n",
        "llm.invoke(\"What is the capital of France?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbO1K21zqh0s",
        "outputId": "2c7b7fb7-aca9-46bd-d0db-52ee976c5a9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# Impact of million-plus token context window language models on Retrieval-Augmented Generation (RAG)\n",
            "\n",
            "## Introduction\n",
            "\n",
            "An overview of the topic, including a definition of Retrieval-Augmented Generation (RAG) and a brief explanation of million-plus token context window language models.\n",
            "\n",
            "## Background\n",
            "\n",
            "A history of language models and retrieval-based systems, leading up to the development of RAG and million-plus token context window language models.\n",
            "\n",
            "## Million-Plus Token Context Window Language Models\n",
            "\n",
            "A detailed explanation of million-plus token context window language models, including their architecture, capabilities, and limitations.\n",
            "\n",
            "### Architecture\n",
            "\n",
            "A description of the internal structure and components of million-plus token context window language models.\n",
            "\n",
            "### Capabilities\n",
            "\n",
            "An explanation of what million-plus token context window language models can do, including examples of tasks they can perform.\n",
            "\n",
            "### Limitations\n",
            "\n",
            "A discussion of the limitations of million-plus token context window language models, including any challenges they face in terms of computational resources, data availability, or other factors.\n",
            "\n",
            "## Retrieval-Augmented Generation (RAG)\n",
            "\n",
            "A detailed explanation of RAG, including its architecture, capabilities, and limitations.\n",
            "\n",
            "### Architecture\n",
            "\n",
            "A description of the internal structure and components of RAG.\n",
            "\n",
            "### Capabilities\n",
            "\n",
            "An explanation of what RAG can do, including examples of tasks it can perform.\n",
            "\n",
            "### Limitations\n",
            "\n",
            "A discussion of the limitations of RAG, including any challenges it faces in terms of computational resources, data availability, or other factors.\n",
            "\n",
            "## Impact of Million-Plus Token Context Window Language Models on RAG\n",
            "\n",
            "A discussion of how million-plus token context window language models have impacted RAG, including any improvements or challenges they have introduced.\n",
            "\n",
            "## Conclusion\n",
            "\n",
            "A summary of the main points of the article and any implications for the future of RAG and language models in general.\n"
          ]
        }
      ],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "direct_gen_outline_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a Wikipedia writer. Write an outline for a Wikipedia page about a user-provided topic. Be comprehensive and specific.\",\n",
        "        ),\n",
        "        (\"user\", \"{topic}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "class Subsection(BaseModel):\n",
        "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
        "    description: str = Field(..., title=\"Content of the subsection\")\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        return f\"### {self.subsection_title}\\n\\n{self.description}\".strip()\n",
        "\n",
        "\n",
        "class Section(BaseModel):\n",
        "    section_title: str = Field(..., title=\"Title of the section\")\n",
        "    description: str = Field(..., title=\"Content of the section\")\n",
        "    subsections: Optional[List[Subsection]] = Field(\n",
        "        default=None,\n",
        "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        subsections = \"\\n\\n\".join(\n",
        "            f\"### {subsection.subsection_title}\\n\\n{subsection.description}\"\n",
        "            for subsection in self.subsections or []\n",
        "        )\n",
        "        return f\"## {self.section_title}\\n\\n{self.description}\\n\\n{subsections}\".strip()\n",
        "\n",
        "\n",
        "class Outline(BaseModel):\n",
        "    page_title: str = Field(..., title=\"Title of the Wikipedia page\")\n",
        "    sections: List[Section] = Field(\n",
        "        default_factory=list,\n",
        "        title=\"Titles and descriptions for each section of the Wikipedia page.\",\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        sections = \"\\n\\n\".join(section.as_str for section in self.sections)\n",
        "        return f\"# {self.page_title}\\n\\n{sections}\".strip()\n",
        "\n",
        "\n",
        "generate_outline_direct = direct_gen_outline_prompt | llm.with_structured_output(\n",
        "    Outline\n",
        ")\n",
        "example_topic = \"Impact of million-plus token context window language models on RAG\"\n",
        "\n",
        "initial_outline = generate_outline_direct.invoke({\"topic\": example_topic})\n",
        "\n",
        "print(initial_outline.as_str)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# We will start our search by generating a list of related topics, sourced from Wikipedia.\n",
        "\n",
        "gen_related_topics_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"I'm writing a Wikipedia page for a topic mentioned below. Please identify and recommend some Wikipedia pages on closely related subjects. I'm looking for examples that provide insights into interesting aspects commonly associated with this topic, or examples that help me understand the typical content and structure included in Wikipedia pages for similar topics.\n",
        "\n",
        "Please list only FOUR \" 4 \" subjects(compulsory) and urls(compulsory) as you can.\n",
        "\n",
        "Topic of interest: {topic}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "class RelatedSubjects(BaseModel):\n",
        "    topics: List[str] = Field(\n",
        "        description=\"Comprehensive list of related subjects as background research.\",\n",
        "    )\n",
        "\n",
        "\n",
        "expand_chain = gen_related_topics_prompt | llm.with_structured_output(\n",
        "    RelatedSubjects\n",
        ")\n",
        "related_subjects = await expand_chain.ainvoke({\"topic\": example_topic})\n",
        "related_subjects"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwAdiqAUdvaT",
        "outputId": "2edf90b8-bc27-40db-946c-82bb6202cd35"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RelatedSubjects(topics=['Language models', 'Retrieval-augmented generation', 'Transformer models', 'Natural language processing'])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Perspectives\n",
        "# From these related subjects, we can select representative Wikipedia editors as \"subject matter experts\" with distinct backgrounds and affiliations. These will help distribute the search process to encourage a more well-rounded final report.\n",
        "\n",
        "class Editor(BaseModel):\n",
        "    affiliation: str = Field(\n",
        "        description=\"Primary affiliation of the editor.\",\n",
        "    )\n",
        "    name: str = Field(\n",
        "        description=\"Name of the editor.\", pattern=r\"^[a-zA-Z0-9_-]{1,64}$\"\n",
        "    )\n",
        "    role: str = Field(\n",
        "        description=\"Role of the editor in the context of the topic.\",\n",
        "    )\n",
        "    description: str = Field(\n",
        "        description=\"Description of the editor's focus, concerns, and motives.\",\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def persona(self) -> str:\n",
        "        return f\"Name: {self.name}\\nRole: {self.role}\\nAffiliation: {self.affiliation}\\nDescription: {self.description}\\n\"\n",
        "\n",
        "\n",
        "class Perspectives(BaseModel):\n",
        "    editors: List[Editor] = Field(\n",
        "        description=\"Comprehensive list of editors with their roles and affiliations.\",\n",
        "        # Add a pydantic validation/restriction to be at most M editors\n",
        "    )\n",
        "\n",
        "\n",
        "gen_perspectives_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You need to select a diverse (and distinct) group of Wikipedia editors who will work together to create a comprehensive article on the topic. Each of them represents a different perspective, role, or affiliation related to this topic.\\\n",
        "    You can use other Wikipedia pages of related topics for inspiration. For each editor, add a description of what they will focus on.\n",
        "\n",
        "    Wiki page outlines of related topics for inspiration:\n",
        "    {examples}\"\"\",\n",
        "        ),\n",
        "        (\"user\", \"Topic of interest: {topic}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gen_perspectives_chain = gen_perspectives_prompt | llm.with_structured_output(Perspectives)\n",
        "\n",
        "from langchain_community.retrievers import WikipediaRetriever\n",
        "from langchain_core.runnables import RunnableLambda\n",
        "from langchain_core.runnables import chain as as_runnable\n",
        "\n",
        "wikipedia_retriever = WikipediaRetriever(load_all_available_meta=True, top_k_results=1)\n",
        "\n",
        "\n",
        "def format_doc(doc, max_length=1000):\n",
        "    related = \"- \".join(doc.metadata[\"categories\"])\n",
        "    return f\"### {doc.metadata['title']}\\n\\nSummary: {doc.page_content}\\n\\nRelated\\n{related}\"[\n",
        "        :1200\n",
        "    ]\n",
        "\n",
        "\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(format_doc(doc) for doc in docs)\n",
        "\n",
        "\n",
        "@as_runnable\n",
        "async def survey_subjects(topic: str):\n",
        "    related_subjects = await expand_chain.ainvoke({\"topic\": topic})\n",
        "    retrieved_docs = await wikipedia_retriever.abatch(\n",
        "        related_subjects.topics, return_exceptions=True\n",
        "    )\n",
        "    all_docs = []\n",
        "    for docs in retrieved_docs:\n",
        "        if isinstance(docs, BaseException):\n",
        "            continue\n",
        "        all_docs.extend(docs)\n",
        "    formatted = format_docs(all_docs)\n",
        "    return await gen_perspectives_chain.ainvoke({\"examples\": formatted, \"topic\": topic})\n",
        "\n",
        "perspectives = await survey_subjects.ainvoke(example_topic)\n",
        "perspectives"
      ],
      "metadata": {
        "id": "5BdTqYF2gzXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "OiKhxPrzsHXx"
      },
      "outputs": [],
      "source": [
        "from typing import Annotated\n",
        "\n",
        "from langchain_core.messages import AnyMessage\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from langgraph.graph import END, StateGraph, START\n",
        "\n",
        "\n",
        "def add_messages(left, right):\n",
        "    if not isinstance(left, list):\n",
        "        left = [left]\n",
        "    if not isinstance(right, list):\n",
        "        right = [right]\n",
        "    return left + right\n",
        "\n",
        "\n",
        "def update_references(references, new_references):\n",
        "    if not references:\n",
        "        references = {}\n",
        "    references.update(new_references)\n",
        "    return references\n",
        "\n",
        "\n",
        "def update_editor(editor, new_editor):\n",
        "    # Can only set at the outset\n",
        "    if not editor:\n",
        "        return new_editor\n",
        "    return editor\n",
        "\n",
        "\n",
        "class InterviewState(TypedDict):\n",
        "    messages: Annotated[List[AnyMessage], add_messages]\n",
        "    references: Annotated[Optional[dict], update_references]\n",
        "    editor: Annotated[Optional[Editor], update_editor]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage, HumanMessage, ToolMessage\n",
        "from langchain_core.prompts import MessagesPlaceholder\n",
        "\n",
        "gen_qn_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an experienced Wikipedia writer and want to edit a specific page. \\\n",
        "Besides your identity as a Wikipedia writer, you have a specific focus when researching the topic. \\\n",
        "Now, you are chatting with an expert to get information. Ask good questions to get more useful information.\n",
        "\n",
        "When you have no more questions to ask, say \"Thank you so much for your help!\" to end the conversation.\\\n",
        "Please only ask one question at a time and don't ask what you have asked before.\\\n",
        "Your questions should be related to the topic you want to write.\n",
        "Be comprehensive and curious, gaining as much unique insight from the expert as possible.\\\n",
        "\n",
        "Stay true to your specific perspective:\n",
        "\n",
        "{persona}\"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def tag_with_name(ai_message: AIMessage, name: str):\n",
        "    ai_message.name = name\n",
        "    return ai_message\n",
        "\n",
        "\n",
        "def swap_roles(state: InterviewState, name: str):\n",
        "    converted = []\n",
        "    for message in state[\"messages\"]:\n",
        "        if isinstance(message, AIMessage) and message.name != name:\n",
        "            message = HumanMessage(**message.dict(exclude={\"type\"}))\n",
        "        converted.append(message)\n",
        "    return {\"messages\": converted}\n",
        "\n",
        "\n",
        "@as_runnable\n",
        "async def generate_question(state: InterviewState):\n",
        "    editor = state[\"editor\"]\n",
        "    gn_chain = (\n",
        "        RunnableLambda(swap_roles).bind(name=editor.name)\n",
        "        | gen_qn_prompt.partial(persona=editor.persona)\n",
        "        | llm\n",
        "        | RunnableLambda(tag_with_name).bind(name=editor.name)\n",
        "    )\n",
        "    result = await gn_chain.ainvoke(state)\n",
        "    return {\"messages\": [result]}\n",
        "messages = [\n",
        "    HumanMessage(f\"So you said you were writing an article on {example_topic}?\")\n",
        "]\n",
        "question = await generate_question.ainvoke(\n",
        "    {\n",
        "        \"editor\": perspectives.editors[0],\n",
        "        \"messages\": messages,\n",
        "    }\n",
        ")\n",
        "\n",
        "question[\"messages\"][0].content\n"
      ],
      "metadata": {
        "id": "JFt6VEfKkAu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "40882792-ba17-4222-d67d-709d8deeb01a"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Yes, that's correct. I'm interested in exploring the technical aspects of how million-plus token context window language models integrate with Retrieval-Augmented Generation (RAG). To start, can you explain what the primary challenges are when integrating language models with such large context windows into RAG?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Queries(BaseModel):\n",
        "    queries: List[str] = Field(\n",
        "        description=\"Comprehensive list of search engine queries to answer the user's questions.\",\n",
        "    )\n",
        "\n",
        "\n",
        "gen_queries_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful research assistant. Query the search engine to answer the user's questions.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "gen_queries_chain = gen_queries_prompt | ChatGroq(\n",
        "    model=\"llama-3.3-70b-versatile\"\n",
        ").with_structured_output(Queries, include_raw=True)\n",
        "queries = await gen_queries_chain.ainvoke(\n",
        "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
        ")\n",
        "queries[\"parsed\"].queries\n",
        "\n",
        "class AnswerWithCitations(BaseModel):\n",
        "    answer: str = Field(\n",
        "        description=\"Comprehensive answer to the user's question with citations.\",\n",
        "    )\n",
        "    cited_urls: List[str] = Field(\n",
        "        description=\"List of urls cited in the answer.\",\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        return f\"{self.answer}\\n\\nCitations:\\n\\n\" + \"\\n\".join(\n",
        "            f\"[{i+1}]: {url}\" for i, url in enumerate(self.cited_urls)\n",
        "        )\n",
        "\n",
        "\n",
        "gen_answer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are an expert who can use information effectively. You are chatting with a Wikipedia writer who wants\\\n",
        " to write a Wikipedia page on the topic you know. You have gathered the related information and will now use the information to form a response.\n",
        "\n",
        "Make your response as informative as possible and make sure every sentence is supported by the gathered information.\n",
        "Each response must be backed up by a citation from a reliable source, formatted as a footnote, reproducing the URLS after your response.\"\"\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"messages\", optional=True),\n",
        "    ]\n",
        ")\n",
        "\n",
        "gen_answer_chain = gen_answer_prompt | llm.with_structured_output(\n",
        "    AnswerWithCitations, include_raw=True\n",
        ").with_config(run_name=\"GenerateAnswer\")\n",
        "from langchain_community.utilities.duckduckgo_search import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "'''\n",
        "# Tavily is typically a better search engine, but your free queries are limited\n",
        "search_engine = TavilySearchResults(max_results=4)\n",
        "\n",
        "@tool\n",
        "async def search_engine(query: str):\n",
        "    \"\"\"Search engine to the internet.\"\"\"\n",
        "    results = tavily_search.invoke(query)\n",
        "    return [{\"content\": r[\"content\"], \"url\": r[\"url\"]} for r in results]\n",
        "'''\n",
        "\n",
        "# DDG\n",
        "search_engine = DuckDuckGoSearchAPIWrapper()\n",
        "\n",
        "\n",
        "@tool\n",
        "async def search_engine(query: str):\n",
        "    \"\"\"Search engine to the internet.\"\"\"\n",
        "    results = DuckDuckGoSearchAPIWrapper()._ddgs_text(query)\n",
        "    return [{\"content\": r[\"body\"], \"url\": r[\"href\"]} for r in results]\n",
        "import json\n",
        "\n",
        "from langchain_core.runnables import RunnableConfig\n",
        "\n",
        "\n",
        "async def gen_answer(\n",
        "    state: InterviewState,\n",
        "    config: Optional[RunnableConfig] = None,\n",
        "    name: str = \"Subject_Matter_Expert\",\n",
        "    max_str_len: int = 15000,\n",
        "):\n",
        "    swapped_state = swap_roles(state, name)  # Convert all other AI messages\n",
        "    queries = await gen_queries_chain.ainvoke(swapped_state)\n",
        "    query_results = await search_engine.abatch(\n",
        "        queries[\"parsed\"].queries, config, return_exceptions=True\n",
        "    )\n",
        "    successful_results = [\n",
        "        res for res in query_results if not isinstance(res, Exception)\n",
        "    ]\n",
        "    all_query_results = {\n",
        "        res[\"url\"]: res[\"content\"] for results in successful_results for res in results\n",
        "    }\n",
        "    # We could be more precise about handling max token length if we wanted to here\n",
        "    dumped = json.dumps(all_query_results)[:max_str_len]\n",
        "    ai_message: AIMessage = queries[\"raw\"]\n",
        "    tool_call = queries[\"raw\"].tool_calls[0]\n",
        "    tool_id = tool_call[\"id\"]\n",
        "    tool_message = ToolMessage(tool_call_id=tool_id, content=dumped)\n",
        "    swapped_state[\"messages\"].extend([ai_message, tool_message])\n",
        "    # Only update the shared state with the final answer to avoid\n",
        "    # polluting the dialogue history with intermediate messages\n",
        "    generated = await gen_answer_chain.ainvoke(swapped_state)\n",
        "    cited_urls = set(generated[\"parsed\"].cited_urls)\n",
        "    # Save the retrieved information to a the shared state for future reference\n",
        "    cited_references = {k: v for k, v in all_query_results.items() if k in cited_urls}\n",
        "    formatted_message = AIMessage(name=name, content=generated[\"parsed\"].as_str)\n",
        "    return {\"messages\": [formatted_message], \"references\": cited_references}\n",
        "example_answer = await gen_answer(\n",
        "    {\"messages\": [HumanMessage(content=question[\"messages\"][0].content)]}\n",
        ")\n",
        "example_answer[\"messages\"][-1].content\n"
      ],
      "metadata": {
        "id": "zYnsDLnfmwAh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "max_num_turns = 5\n",
        "from langgraph.pregel import RetryPolicy\n",
        "\n",
        "\n",
        "def route_messages(state: InterviewState, name: str = \"Subject_Matter_Expert\"):\n",
        "    messages = state[\"messages\"]\n",
        "    num_responses = len(\n",
        "        [m for m in messages if isinstance(m, AIMessage) and m.name == name]\n",
        "    )\n",
        "    if num_responses >= max_num_turns:\n",
        "        return END\n",
        "    last_question = messages[-2]\n",
        "    if last_question.content.endswith(\"Thank you so much for your help!\"):\n",
        "        return END\n",
        "    return \"ask_question\"\n",
        "\n",
        "\n",
        "builder = StateGraph(InterviewState)\n",
        "\n",
        "builder.add_node(\"ask_question\", generate_question, retry=RetryPolicy(max_attempts=5))\n",
        "builder.add_node(\"answer_question\", gen_answer, retry=RetryPolicy(max_attempts=5))\n",
        "builder.add_conditional_edges(\"answer_question\", route_messages)\n",
        "builder.add_edge(\"ask_question\", \"answer_question\")\n",
        "\n",
        "builder.add_edge(START, \"ask_question\")\n",
        "interview_graph = builder.compile(checkpointer=False).with_config(\n",
        "    run_name=\"Conduct Interviews\"\n",
        ")\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(interview_graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass\n",
        "\n",
        "final_step = None\n",
        "\n",
        "initial_state = {\n",
        "    \"editor\": perspectives.editors[0],\n",
        "    \"messages\": [\n",
        "        AIMessage(\n",
        "            content=f\"So you said you were writing an article on {example_topic}?\",\n",
        "            name=\"Subject_Matter_Expert\",\n",
        "        )\n",
        "    ],\n",
        "}\n",
        "async for step in interview_graph.astream(initial_state):\n",
        "    name = next(iter(step))\n",
        "\n",
        "\n",
        "refine_outline_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"\"\"You are a Wikipedia writer. You have gathered information from experts and search engines. Now, you are refining the outline of the Wikipedia page. \\\n",
        "You need to make sure that the outline is comprehensive and specific. \\\n",
        "Topic you are writing about: {topic}\n",
        "\n",
        "Old outline:\n",
        "\n",
        "{old_outline}\"\"\",\n",
        "        ),\n",
        "        (\n",
        "            \"user\",\n",
        "            \"Refine the outline based on your conversations with subject-matter experts:\\n\\nConversations:\\n\\n{conversations}\\n\\nWrite the refined Wikipedia outline:\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Using turbo preview since the context can get quite long\n",
        "refine_outline_chain = refine_outline_prompt | llm.with_structured_output(\n",
        "    Outline\n",
        ")"
      ],
      "metadata": {
        "id": "OY7Xhva9m3dt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_step = step\n",
        "final_state = next(iter(final_step.values()))\n",
        "\n",
        "refined_outline = refine_outline_chain.invoke(\n",
        "    {\n",
        "        \"topic\": example_topic,\n",
        "        \"old_outline\": initial_outline.as_str,\n",
        "        \"conversations\": \"\\n\\n\".join(\n",
        "            f\"### {m.name}\\n\\n{m.content}\" for m in final_state[\"messages\"]\n",
        "        ),\n",
        "    }\n",
        ")\n"
      ],
      "metadata": {
        "id": "nU3fZ0RsxBfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
        "model_kwargs = {'device': 'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings': False}\n",
        "hf = HuggingFaceEmbeddings(\n",
        "    model_name=model_name,\n",
        "    model_kwargs=model_kwargs,\n",
        "    encode_kwargs=encode_kwargs\n",
        ")"
      ],
      "metadata": {
        "id": "hgL51OeHyfRS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import InMemoryVectorStore\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "\n",
        "\n",
        "reference_docs = [\n",
        "    Document(page_content=v, metadata={\"source\": k})\n",
        "    for k, v in final_state[\"references\"].items()\n",
        "]\n",
        "# This really doesn't need to be a vectorstore for this size of data.\n",
        "# It could just be a numpy matrix. Or you could store documents\n",
        "# across requests if you want.\n",
        "vectorstore = InMemoryVectorStore.from_documents(\n",
        "    reference_docs,\n",
        "    embedding=hf,\n",
        ")\n",
        "retriever = vectorstore.as_retriever(k=3)\n",
        "retriever.invoke(\"What's a long context LLM anyway?\")"
      ],
      "metadata": {
        "id": "pmPsVmgKwzsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SubSection(BaseModel):\n",
        "    subsection_title: str = Field(..., title=\"Title of the subsection\")\n",
        "    content: str = Field(\n",
        "        ...,\n",
        "        title=\"Full content of the subsection. Include [#] citations to the cited sources where relevant.\",\n",
        "    )\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        return f\"### {self.subsection_title}\\n\\n{self.content}\".strip()\n",
        "\n",
        "\n",
        "class WikiSection(BaseModel):\n",
        "    section_title: str = Field(..., title=\"Title of the section\")\n",
        "    content: str = Field(..., title=\"Full content of the section\")\n",
        "    subsections: Optional[List[Subsection]] = Field(\n",
        "        default=None,\n",
        "        title=\"Titles and descriptions for each subsection of the Wikipedia page.\",\n",
        "    )\n",
        "    citations: List[str] = Field(default_factory=list)\n",
        "\n",
        "    @property\n",
        "    def as_str(self) -> str:\n",
        "        subsections = \"\\n\\n\".join(\n",
        "            subsection.as_str for subsection in self.subsections or []\n",
        "        )\n",
        "        citations = \"\\n\".join([f\" [{i}] {cit}\" for i, cit in enumerate(self.citations)])\n",
        "        return (\n",
        "            f\"## {self.section_title}\\n\\n{self.content}\\n\\n{subsections}\".strip()\n",
        "            + f\"\\n\\n{citations}\".strip()\n",
        "        )\n",
        "\n",
        "\n",
        "section_writer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert Wikipedia writer. Complete your assigned WikiSection from the following outline:\\n\\n\"\n",
        "            \"{outline}\\n\\nCite your sources, using the following references:\\n\\n<Documents>\\n{docs}\\n<Documents>\",\n",
        "        ),\n",
        "        (\"user\", \"Write the full WikiSection for the {section} section.\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "async def retrieve(inputs: dict):\n",
        "    docs = await retriever.ainvoke(inputs[\"topic\"] + \": \" + inputs[\"section\"])\n",
        "    formatted = \"\\n\".join(\n",
        "        [\n",
        "            f'<Document href=\"{doc.metadata[\"source\"]}\"/>\\n{doc.page_content}\\n</Document>'\n",
        "            for doc in docs\n",
        "        ]\n",
        "    )\n",
        "    return {\"docs\": formatted, **inputs}\n",
        "\n",
        "\n",
        "section_writer = (\n",
        "    retrieve\n",
        "    | section_writer_prompt\n",
        "    | llm.with_structured_output(WikiSection)\n",
        ")\n",
        "section = await section_writer.ainvoke(\n",
        "    {\n",
        "        \"outline\": refined_outline.as_str,\n",
        "        \"section\": refined_outline.sections[1].section_title,\n",
        "        \"topic\": example_topic,\n",
        "    }\n",
        ")\n",
        "print(section.as_str)"
      ],
      "metadata": {
        "id": "tE_L7WquzGrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "writer_prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are an expert Wikipedia author. Write the complete wiki article on {topic} using the following section drafts:\\n\\n\"\n",
        "            \"{draft}\\n\\nStrictly follow Wikipedia format guidelines.\",\n",
        "        ),\n",
        "        (\n",
        "            \"user\",\n",
        "            'Write the complete Wiki article using markdown format. Organize citations using footnotes like \"[1]\",'\n",
        "            \" avoiding duplicates in the footer. Include URLs in the footer.\",\n",
        "        ),\n",
        "    ]\n",
        ")\n",
        "\n",
        "writer = writer_prompt | model | StrOutputParser()\n",
        "# for tok in writer.stream({\"topic\": example_topic, \"draft\": section.as_str}):\n",
        "#     print(tok, end=\"\")"
      ],
      "metadata": {
        "id": "u-HKVj6o2gdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ResearchState(TypedDict):\n",
        "    topic: str\n",
        "    outline: Outline\n",
        "    editors: List[Editor]\n",
        "    interview_results: List[InterviewState]\n",
        "    # The final sections output\n",
        "    sections: List[WikiSection]\n",
        "    article: str\n",
        "import asyncio\n",
        "\n",
        "\n",
        "async def initialize_research(state: ResearchState):\n",
        "    topic = state[\"topic\"]\n",
        "    coros = (\n",
        "        generate_outline_direct.ainvoke({\"topic\": topic}),\n",
        "        survey_subjects.ainvoke(topic),\n",
        "    )\n",
        "    results = await asyncio.gather(*coros)\n",
        "    return {\n",
        "        **state,\n",
        "        \"outline\": results[0],\n",
        "        \"editors\": results[1].editors,\n",
        "    }\n",
        "\n",
        "\n",
        "async def conduct_interviews(state: ResearchState):\n",
        "    topic = state[\"topic\"]\n",
        "    initial_states = [\n",
        "        {\n",
        "            \"editor\": editor,\n",
        "            \"messages\": [\n",
        "                AIMessage(\n",
        "                    content=f\"So you said you were writing an article on {topic}?\",\n",
        "                    name=\"Subject_Matter_Expert\",\n",
        "                )\n",
        "            ],\n",
        "        }\n",
        "        for editor in state[\"editors\"]\n",
        "    ]\n",
        "    # We call in to the sub-graph here to parallelize the interviews\n",
        "    interview_results = await interview_graph.abatch(initial_states)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"interview_results\": interview_results,\n",
        "    }\n",
        "\n",
        "\n",
        "def format_conversation(interview_state):\n",
        "    messages = interview_state[\"messages\"]\n",
        "    convo = \"\\n\".join(f\"{m.name}: {m.content}\" for m in messages)\n",
        "    return f'Conversation with {interview_state[\"editor\"].name}\\n\\n' + convo\n",
        "\n",
        "\n",
        "async def refine_outline(state: ResearchState):\n",
        "    convos = \"\\n\\n\".join(\n",
        "        [\n",
        "            format_conversation(interview_state)\n",
        "            for interview_state in state[\"interview_results\"]\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    updated_outline = await refine_outline_chain.ainvoke(\n",
        "        {\n",
        "            \"topic\": state[\"topic\"],\n",
        "            \"old_outline\": state[\"outline\"].as_str,\n",
        "            \"conversations\": convos,\n",
        "        }\n",
        "    )\n",
        "    return {**state, \"outline\": updated_outline}\n",
        "\n",
        "\n",
        "async def index_references(state: ResearchState):\n",
        "    all_docs = []\n",
        "    for interview_state in state[\"interview_results\"]:\n",
        "        reference_docs = [\n",
        "            Document(page_content=v, metadata={\"source\": k})\n",
        "            for k, v in interview_state[\"references\"].items()\n",
        "        ]\n",
        "        all_docs.extend(reference_docs)\n",
        "    await vectorstore.aadd_documents(all_docs)\n",
        "    return state\n",
        "\n",
        "\n",
        "async def write_sections(state: ResearchState):\n",
        "    outline = state[\"outline\"]\n",
        "    sections = await section_writer.abatch(\n",
        "        [\n",
        "            {\n",
        "                \"outline\": refined_outline.as_str,\n",
        "                \"section\": section.section_title,\n",
        "                \"topic\": state[\"topic\"],\n",
        "            }\n",
        "            for section in outline.sections\n",
        "        ]\n",
        "    )\n",
        "    return {\n",
        "        **state,\n",
        "        \"sections\": sections,\n",
        "    }\n",
        "\n",
        "\n",
        "async def write_article(state: ResearchState):\n",
        "    topic = state[\"topic\"]\n",
        "    sections = state[\"sections\"]\n",
        "    draft = \"\\n\\n\".join([section.as_str for section in sections])\n",
        "    article = await writer.ainvoke({\"topic\": topic, \"draft\": draft})\n",
        "    return {\n",
        "        **state,\n",
        "        \"article\": article,\n",
        "    }\n",
        "\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "builder_of_storm = StateGraph(ResearchState)\n",
        "\n",
        "nodes = [\n",
        "    (\"init_research\", initialize_research),\n",
        "    (\"conduct_interviews\", conduct_interviews),\n",
        "    (\"refine_outline\", refine_outline),\n",
        "    (\"index_references\", index_references),\n",
        "    (\"write_sections\", write_sections),\n",
        "    (\"write_article\", write_article),\n",
        "]\n",
        "for i in range(len(nodes)):\n",
        "    name, node = nodes[i]\n",
        "    builder_of_storm.add_node(name, node, retry=RetryPolicy(max_attempts=3))\n",
        "    if i > 0:\n",
        "        builder_of_storm.add_edge(nodes[i - 1][0], name)\n",
        "\n",
        "builder_of_storm.add_edge(START, nodes[0][0])\n",
        "builder_of_storm.add_edge(nodes[-1][0], END)\n",
        "storm = builder_of_storm.compile(checkpointer=MemorySaver())\n",
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(storm.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "70W4ZXGj2ltW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"thread_id\": \"my-thread\"}}\n",
        "async for step in storm.astream(\n",
        "    {\n",
        "        \"topic\": \"Groq, NVIDIA, Llamma.cpp and the future of LLM Inference\",\n",
        "    },\n",
        "    config,\n",
        "):\n",
        "    name = next(iter(step))\n",
        "\n"
      ],
      "metadata": {
        "id": "GkpAG1Mr3aEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "checkpoint = storm.get_state(config)\n",
        "article = checkpoint.values[\"article\"]\n",
        "\n",
        "from IPython.display import Markdown\n",
        "\n",
        "# We will down-header the sections to create less confusion in this notebook\n",
        "Markdown(article.replace(\"\\n#\", \"\\n##\"))"
      ],
      "metadata": {
        "id": "aPog-oYB4X4J"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNxVOgKOriQehrjqJJU2k0v",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}